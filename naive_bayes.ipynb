{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯的实现并用于垃圾邮寄分类测试\n",
    "朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y。\n",
    "朴素贝叶斯法实际学习到生成数据的机制，即数据特征和类别的联合概率分布 P(X,Y)，所以属于生成模型。\n",
    "\n",
    "***\n",
    "## 贝叶斯公式与朴素贝叶斯模型\n",
    "$$ P(Y|X) = \\frac{P(Y)P(X|Y)}{P(X)} $$\n",
    "\n",
    "通过贝叶斯公式可以看出，朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型九三后验概率分布$ P(Y=c_k|X=x) $, 将后验概率最大的类作为x的类输出。\n",
    "\n",
    "后验概率计算根据贝叶斯公式可得：\n",
    "\n",
    "$$ P(Y=c_k|X=x)=\\frac{P(Y=c_k)P(X=x|Y=c_k)}{\\sum_kP(Y=c_k)P(X=x|Y=c_k)} $$\n",
    "\n",
    "因为对于每个样本 p(x)的总是相同的，所以在不同类别的后验概率的计算中分母是相同的，可以省略掉。这样贝叶斯分类器可以表示为：\n",
    "\n",
    "$$ y=argmax_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k) $$ 其中 j 代表X特征个数，k代表类别的个数\n",
    "\n",
    "***\n",
    "## 后验概率最大化的含义\n",
    "在朴素贝叶斯中，将实例分到后验概率最大的类中，这等价于期望风险最小化。\n",
    "\n",
    "***\n",
    "## 朴素贝叶斯法的参数估计\n",
    "* #### 极大似然估计法\n",
    "先验概率的极大似然估计为： $ P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,...,K $ 其中 I 是boolean函数\n",
    "\n",
    "    设第 j 个特征 $ x^{(j)} $ 可能取值的集合为 {$ a_{j1}, a_{j2},...,a_{js_j} $}，则条件概率的极大似然估计为\n",
    "    \n",
    "     $$ P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} $$\n",
    "    其中 j=1,2,...,n;  l=1,2,...,Sj;  k=1,2,...,K\n",
    "    \n",
    "* #### 贝叶斯估计法\n",
    "用极大似然估计可能会出现某个所有估计的条件概率为0，这时会导致后延概率的计算结果为0，是分类不准确。为了解决这一问题，可以采用贝叶斯估计。\n",
    "\n",
    "    给定 $ \\lambda>0 $，则先验概率的贝叶斯估计为：\n",
    "$$ P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)+\\lambda}{N+K\\lambda} $$\n",
    "\n",
    "    条件概率的贝叶斯估计为：\n",
    "    $$ P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda} $$\n",
    "    \n",
    "    其中 Sj 为第j个特征的取值个数。可以看出当 $ \\lambda=0 $ 时就变成了极大似然估计，特殊的当 $ \\lambda=1 $,就是拉普拉斯平滑，这样就解决了在极大似然估计中概率为0的问题。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 导入相关包\n",
    "这里导入 sklearn中的datasets包用于下载fetch_20newsgroups 数据作为本次模型的训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(data_home=\"./data/\", subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 构建一个辅助函数用于处理文本中的符号，并切词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理文本中的符号\n",
    "def process_text(data):\n",
    "    processed_data = []\n",
    "    for example in data:\n",
    "        example = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，\\n。？、~@#￥%……&*（）]+\", \" \", example)\n",
    "        example = re.sub(r\"\\W\", \" \", example)\n",
    "        processed_data.append(example.lower().split())\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 划分数据集\n",
    "使用 train_test_split()函数划分训练集和测试集\n",
    "\n",
    "并对训练和测试数据集的文本进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14134, 14134), (4712, 4712))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)\n",
    "\n",
    "((len(X_train), len(Y_train)), (len(X_test), len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理文本\n",
    "X_train = process_text(X_train)\n",
    "X_test = process_text(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建朴素贝叶斯模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, lambd):\n",
    "        self.lambd = lambd\n",
    "        self.vocabs = None\n",
    "        self.prior_prob = None\n",
    "        self.conditional_prob = None\n",
    "    \n",
    "    def fit(self, data_x, data_y):\n",
    "        \"\"\"训练，为了计算先验概率与条件概率\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"开始构建词表...\")\n",
    "        self.vocabs = self.generate_vocab(data_x)\n",
    "        print(\"生成训练数据矩阵\")\n",
    "        data_vec = self.convert_data_to_vec(data_x, self.vocabs)\n",
    "        \n",
    "        cate_num_k = len(set(data_y))\n",
    "        \n",
    "        # 计算先验概率\n",
    "        print(\"开始计算先验概率...\")\n",
    "        self.prior_prob = {}\n",
    "        for cate in set(data_y):\n",
    "            # 采用贝叶斯估计\n",
    "            self.prior_prob[cate] = (data_y.count(cate) + self.lambd) / (len(data_y) + cate_num_k * self.lambd)\n",
    "        \n",
    "        # 将数据按照类别不同划分为多个组\n",
    "        group_data = {} \n",
    "        for cate in set(data_y):      \n",
    "            sub_data_x = []\n",
    "            for idx, example_label in enumerate(data_y):\n",
    "                if example_label == cate:\n",
    "                    sub_data_x.append(data_vec[idx])\n",
    "            group_data[cate] = np.asarray(sub_data_x)\n",
    "        \n",
    "        # 计算每个类别的特征的条件概率\n",
    "        print(\"开始计算条件概率...\")\n",
    "        # 所有类别下所有特征的不同取值的条件概率。\n",
    "        self.conditional_prob = {}\n",
    "        for cate in set(data_y):\n",
    "            cate_data = group_data[cate]  \n",
    "            # 某类子数据集的样本个数\n",
    "            num_cate = cate_data.shape[0]\n",
    "         \n",
    "            feature_one_cond_prob = (np.sum(cate_data, axis=0) + self.lambd) / (num_cate + np.array([2] * cate_data.shape[1]) * self.lambd)\n",
    "            feature_zero_cond_prob = (np.array([cate_data.shape[0]] * cate_data.shape[1]) - np.sum(cate_data, axis=0)) / (num_cate + np.array([2] * cate_data.shape[1]) * self.lambd)\n",
    "                \n",
    "            self.conditional_prob[cate] = [feature_zero_cond_prob, feature_one_cond_prob]\n",
    "        stop_time = time.time()\n",
    "        print(\"训练结束，耗时：{0} 秒\".format(str(stop_time-start_time)))\n",
    "        \n",
    "        return self.prior_prob, self.conditional_prob\n",
    "        \n",
    "    \n",
    "    def predict(self, data_test):\n",
    "        \"\"\"预测，在新的数据集上\"\"\"\n",
    "        \n",
    "        if self.prior_prob is None or self.conditional_prob is None or self.vocabs is None:\n",
    "            raise NameError(\"模型未训练，没有可用的参数\")\n",
    "        test_data_vec = self.convert_data_to_vec(data_test, self.vocabs)\n",
    "        \n",
    "        # 测试集在每个类别上的后验概率\n",
    "        test_cate_prob = np.zeros((test_data_vec.shape[0], len(self.prior_prob)))\n",
    "        \n",
    "        cate_idx = 0\n",
    "        # 创建一个类别名称的列表，用于之后对结果的索引\n",
    "        cates_name = []\n",
    "        # 计算测试集每个样本在每个类别上的后验概率\n",
    "        for cate in self.prior_prob.keys():\n",
    "            cate_prior_prob = self.prior_prob[cate]\n",
    "            cate_features_conditional_prob = self.conditional_prob[cate]\n",
    "            \n",
    "            cate_test_data_cond_prob = []\n",
    "            \n",
    "            for example in test_data_vec:\n",
    "                example_feature_prob = 0.0\n",
    "                for idx, feature_value in enumerate(example.tolist()):\n",
    "                    example_feature_prob += cate_features_conditional_prob[int(feature_value)][idx]\n",
    "                    \n",
    "                cate_test_data_cond_prob.append(example_feature_prob)\n",
    "            log_cate_union_prob = np.log(np.asarray(cate_test_data_cond_prob) + cate_prior_prob)\n",
    "            test_cate_prob[:, cate_idx] = log_cate_union_prob\n",
    "            cates_name.append(cate)        \n",
    "            cate_idx += 1\n",
    "            \n",
    "        # 取后验概率最大的索引\n",
    "        argmax_idx = np.argmax(test_cate_prob, axis=1)\n",
    "        # 索引出类别名称\n",
    "        test_cate_result = [cates_name[idx] for idx in argmax_idx]\n",
    "        \n",
    "        return test_cate_result\n",
    "    \n",
    "    def generate_vocab(self, data_x):\n",
    "        \"\"\"生成词表\"\"\"\n",
    "        vocabs = set([])\n",
    "        for example in data_x:\n",
    "            vocabs = vocabs | set(example)\n",
    "        return list(vocabs)\n",
    "    \n",
    "    def convert_data_to_vec(self, data_x, vocabs):\n",
    "        \"\"\"将文本数据集转换为向量，得到数据集矩阵，矩阵高为文本个数，宽为词汇表大小\"\"\"\n",
    "        data_vec = np.zeros((len(data_x), len(vocabs)))\n",
    "        for row, example in enumerate(data_x):\n",
    "            for column, word in enumerate(example):\n",
    "                if word in vocabs:\n",
    "                    data_vec[row][column] = 1\n",
    "        return data_vec\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个 朴素贝叶斯分类器\n",
    "NBClassifier = NaiveBayesClassifier(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建词表...\n",
      "生成训练数据矩阵\n",
      "开始计算先验概率...\n",
      "开始计算条件概率...\n",
      "训练结束，耗时：435.9030749797821 秒\n"
     ]
    }
   ],
   "source": [
    "prior_prob, conditional_prob = NBClassifier.fit(X_train[0:2000], Y_train.tolist()[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
